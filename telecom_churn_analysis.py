# -*- coding: utf-8 -*-
"""Telecom_churn_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBKA9bSJ7QykVnXAsTc-AsCNkiLpVDUD

# **PROJECT**
"""

# Commented out IPython magic to ensure Python compatibility.
#importing libraries
!pip install catboost
!pip install ipython-autotime
# %load_ext autotime

import pandas as pd
import numpy as np
import chardet
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import boxcox
from sklearn.preprocessing import PowerTransformer
from sklearn.preprocessing import QuantileTransformer
from scipy.stats.mstats import winsorize
from scipy.stats import linregress
from statsmodels.stats.outliers_influence import variance_inflation_factor
from scipy.stats import zscore
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
import pickle
import os
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegressionCV
from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedShuffleSplit

warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/amritansh_datasets/Classification projects/Telecom churn analysis/WA_Fn-UseC_-Telco-Customer-Churn.csv')

df.head()

df.info()

df.shape

"""# **EDA**"""

numerical_features=['tenure',	'MonthlyCharges', 'TotalCharges']

# Get all features from the DataFrame
all_features = df.columns.tolist()

# Use set difference to find categorical features
categorical_features = set(all_features) - set(numerical_features)

# Convert the set back to a list
categorical_features = list(categorical_features)

# Print the categorical features
print(categorical_features)

len(categorical_features)

# Drop rows with any NaN values (modifies df in-place)
df.dropna(inplace=True)

df.shape

df.describe()

df.describe(include='object')

# Remove 'Churn'
categorical_features = [feature for feature in categorical_features if feature != 'Churn']

# Count churned and not churned customers
churn_counts = df['Churn'].value_counts()
print(churn_counts)
# Create a bar chart
plt.figure(figsize=(8, 6))  # Set the figure size
churn_counts.plot(kind='bar', color=['blue', 'red'])  # Set colors for churned and not churned
plt.title('Churn Distribution')
plt.xlabel('Churn')
plt.ylabel('Count')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add a grid for better visualization

# Display the chart
plt.show()

# Create a pie chart with a modified color palette
plt.figure(figsize=(8, 6))
plt.pie(churn_counts, labels=churn_counts.index, autopct="%1.1f%%", startangle=140, colors=['green', 'red'])  # Changed 'blue' to 'green'

# Add a title
plt.title('Churn Distribution')

# Display the chart
plt.show()

# Create a distribution plot for each numerical feature
for feature in numerical_features:
    sns.displot(df[feature], kde=True)  # kde=True for kernel density estimation
    plt.title(f"Distribution of {feature}")
    plt.xlabel(feature)
    plt.ylabel("Density")
    plt.show()

#Finding the reason for bimodality in tenure and monthlycharges
import seaborn as sns
import matplotlib.pyplot as plt

# List of categorical columns to analyze
categorical_columns = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',
                       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',
                       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',
                       'Contract', 'PaperlessBilling', 'PaymentMethod']

# 1. Tenure Analysis

# Loop through categorical columns and create box plots for tenure
for column in categorical_columns:
    plt.figure(figsize=(10, 6))  # Adjust figure size as needed
    sns.boxplot(x=column, y='tenure', data=df)
    plt.title(f'Tenure Distribution by {column}')
    plt.xticks(rotation=45)  # Rotate x-axis labels if they overlap
    plt.show()

# 2. MonthlyCharges Analysis

# Loop through categorical columns and create box plots for MonthlyCharges
for column in categorical_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=column, y='MonthlyCharges', data=df)
    plt.title(f'Monthly Charges Distribution by {column}')
    plt.xticks(rotation=45)
    plt.show()

"""**Tenure Bimodality**

Contract Type: The box plot "Tenure Distribution by Contract" reveals a clear distinction. Customers with Month-to-Month contracts have significantly lower tenures compared to those with One year or Two year contracts. This suggests that the contract type is a major contributor to the bimodal distribution of tenure, with short-term contracts leading to one peak of lower tenures and longer-term contracts leading to another peak of higher tenures.

Other Factors: While Contract Type seems to be the primary driver, other factors might also play a subtle role. For instance, there are slight differences in tenure distributions across categories like InternetService, OnlineSecurity, and TechSupport. However, these differences are less pronounced compared to the impact of Contract.

**MonthlyCharges Bimodality**

Internet Service: The box plot "Monthly Charges Distribution by InternetService" shows a clear separation. Customers with Fiber optic internet service have notably higher monthly charges compared to those with DSL or No internet service. This indicates that the type of internet service is a key factor contributing to the bimodality in MonthlyCharges, with Fiber optic leading to a peak at higher charges and DSL/No internet service leading to a peak at lower charges.

Additional Services: Other services like OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, and StreamingMovies also seem to influence MonthlyCharges. Customers who subscribe to these services tend to have higher monthly charges, contributing to the rightward skew and potentially influencing the second peak.
"""

# Box plots for numerical features
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="Churn", y=feature, showmeans=True, data=df)
    plt.title(f'Distribution of {feature} by Churn', fontsize=14)
    plt.xlabel('Churn', fontsize=12)
    plt.ylabel(feature, fontsize=12)
    plt.xticks(rotation=90, fontsize=10)  # Rotate x-axis labels by 90 degrees
    plt.show()

excluded_features = ["customerID"]  # Define the features to exclude

# Count plots for single categorical features (excluding specific features)
for feature in categorical_features:
    if feature in df.columns and feature not in excluded_features:  # Check for presence and exclusion
        plt.figure(figsize=(10, 6))

        # Count plot with percentages
        ax = sns.countplot(x=df[feature], order=df[feature].value_counts().index)
        plt.title(f'Count Plot of {feature}')
        plt.xlabel(feature)
        plt.ylabel('Count')

        # Add percentages as text labels
        total = float(len(df[feature]))
        for p in ax.patches:
            percentage = '{:.1f}%'.format(100 * p.get_height() / total)
            x = p.get_x() + p.get_width() / 2 - 0.1
            y = p.get_y() + p.get_height() + 0.02
            ax.annotate(percentage, (x, y), size=12)

        # Make x-axis labels vertical
        plt.xticks(rotation=90)

        plt.show()

excluded_features = ["customerID"]

# Grouped bar plots to compare categorical features against Churn (excluding customerID)
for feature in categorical_features:
    if feature in df.columns and feature not in excluded_features:
        plt.figure(figsize=(10, 6))
        sns.countplot(data=df, x=feature, hue='Churn')
        plt.title(f'Churn Distribution by {feature}', fontsize=14)
        plt.xlabel(feature, fontsize=12)
        plt.ylabel('Count', fontsize=12)
        plt.xticks(rotation=90, fontsize=10)
        plt.show()

for feature in categorical_features:
    if feature in df.columns:
        print(f"\nFeature: {feature}")
        print(df[feature].value_counts())

df.shape

"""# **Data Preprocessing and feature engineering:**

Looking at the df.head(), df.info() the column TotalCharges	looks like a numerical column but its Dtype is object so it needs further inspection. Also there are 21 columns so the columns which are not visible after using df.head() we need to check them further if they are actually objects and dont need to be changed into a different Dtype. In this case only one column i.e., OnlineBackup is not visible.
"""

df['TotalCharges'].unique()

# Replace ',' and ' ' with '.'
df['TotalCharges'] = df['TotalCharges'].astype(str).str.replace(',', '.', regex=False).str.replace(' ', '', regex=False)

# Convert to numeric, coercing errors (invalid entries become NaN)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

df.info()

df['OnlineBackup'].unique() # Just checking because this column is not visible in df.info()

import pandas as pd
import numpy as np

def check_special_values(df):
    # Initialize counts for NaN, inf, -inf, and zero
    nan_count = 0
    inf_count = 0
    neg_inf_count = 0
    zero_count = 0

    # DataFrames to store rows with special values
    rows_with_nan = pd.DataFrame()
    rows_with_inf = pd.DataFrame()
    rows_with_neg_inf = pd.DataFrame()
    rows_with_zero = pd.DataFrame()

    # Select only numeric columns for special value checks
    numeric_df = df.select_dtypes(include=np.number)

    # Loop through each row of the DataFrame
    for index, row in numeric_df.iterrows(): # Iterate over numeric DataFrame
        if row.isna().any():
            nan_count += 1
            rows_with_nan = pd.concat([rows_with_nan, df.loc[[index]]]) # Append from original df

        if np.isinf(row).any():
            if (row == np.inf).any():
                inf_count += 1
                rows_with_inf = pd.concat([rows_with_inf, df.loc[[index]]]) # Append from original df
            if (row == -np.inf).any():
                neg_inf_count += 1
                rows_with_neg_inf = pd.concat([rows_with_neg_inf, df.loc[[index]]]) # Append from original df

        if (row == 0).any():
            zero_count += 1
            rows_with_zero = pd.concat([rows_with_zero, df.loc[[index]]]) # Append from original df

    # Summary of counts
    print(f"NaN count: {nan_count}")
    print(f"inf count: {inf_count}")
    print(f"-inf count: {neg_inf_count}")
    print(f"Zero value count: {zero_count}")

    # Return rows containing special values
    return {
        "rows_with_nan": rows_with_nan,
        "rows_with_inf": rows_with_inf,
        "rows_with_neg_inf": rows_with_neg_inf,
        "rows_with_zero": rows_with_zero
    }

result = check_special_values(df)
print(result)

rows_with_nan = result["rows_with_nan"]
print(rows_with_nan)

df['TotalCharges'].describe()

import pandas as pd

# Step 1: Calculate Q1 (25th percentile) and Q3 (75th percentile)
Q1 = df['TotalCharges'].quantile(0.25)
Q3 = df['TotalCharges'].quantile(0.75)

# Step 2: Calculate the Interquartile Range (IQR)
IQR = Q3 - Q1

# Step 3: Define the lower and upper bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Step 4: Filter out the outliers (without changing the original column)
filtered_total_charges = df['TotalCharges'][(df['TotalCharges'] >= lower_bound) & (df['TotalCharges'] <= upper_bound)]

# Step 5: Calculate the mean of the filtered values
mean_filtered_total_charges = filtered_total_charges.mean()

# Print the results
print(f"Mean of 'TotalCharges' after removing outliers: {mean_filtered_total_charges}")

# Replace NaN values in 'TotalCharges' with the calculated mean
df['TotalCharges'].fillna(mean_filtered_total_charges, inplace=True)

# Verify the replacement
print(df['TotalCharges'].isna().sum())  # This should return 0 if all NaN values are replaced

y=df['Churn']

X = df.drop(columns=['customerID', 'Churn'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'gender' column in the training data
X_train['gender_encoded'] = le.fit_transform(X_train['gender'])

# Transform the 'gender' column in the testing data
X_test['gender_encoded'] = le.transform(X_test['gender'])

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'Partner' column in the training data
X_train['Partner_encoded'] = le.fit_transform(X_train['Partner'])

# Transform the 'Partner' column in the testing data
X_test['Partner_encoded'] = le.transform(X_test['Partner'])

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'Dependents' column in the training data
X_train['Dependents_encoded'] = le.fit_transform(X_train['Dependents'])

# Transform the 'Dependents' column in the testing data
X_test['Dependents_encoded'] = le.transform(X_test['Dependents'])

from sklearn.preprocessing import StandardScaler

# Calculate the median of the 'tenure' column in X_train
median_tenure = X_train['tenure'].median()

# Create a new column 'tenure_mode' in X_train
X_train['tenure_mode'] = (X_train['tenure'] > median_tenure).astype(int)

# Standardize the 'tenure' column in X_train
scaler = StandardScaler()
X_train['tenure_scaled'] = scaler.fit_transform(X_train[['tenure']])

# Repeat for X_test using the median from X_train
X_test['tenure_mode'] = (X_test['tenure'] > median_tenure).astype(int)
X_test['tenure_scaled'] = scaler.transform(X_test[['tenure']])
#  Include both tenure_scaled and tenure_mode in model and drop tenure

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'PhoneService' column in the training data
X_train['PhoneService_encoded'] = le.fit_transform(X_train['PhoneService'])

# Transform the 'PhoneService' column in the testing data
X_test['PhoneService_encoded'] = le.transform(X_test['PhoneService'])

import pandas as pd

# Perform one-hot encoding on 'MultipleLines'
X_train = pd.get_dummies(X_train, columns=['MultipleLines'], prefix='MultipleLines')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['MultipleLines'], prefix='MultipleLines')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'InternetService'
X_train = pd.get_dummies(X_train, columns=['InternetService'], prefix='InternetService')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['InternetService'], prefix='InternetService')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'OnlineSecurity'
X_train = pd.get_dummies(X_train, columns=['OnlineSecurity'], prefix='OnlineSecurity')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['OnlineSecurity'], prefix='OnlineSecurity')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'OnlineBackup'
X_train = pd.get_dummies(X_train, columns=['OnlineBackup'], prefix='OnlineBackup')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['OnlineBackup'], prefix='OnlineBackup')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'DeviceProtection'
X_train = pd.get_dummies(X_train, columns=['DeviceProtection'], prefix='DeviceProtection')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['DeviceProtection'], prefix='DeviceProtection')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'TechSupport'
X_train = pd.get_dummies(X_train, columns=['TechSupport'], prefix='TechSupport')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['TechSupport'], prefix='TechSupport')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'StreamingTV'
X_train = pd.get_dummies(X_train, columns=['StreamingTV'], prefix='StreamingTV')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['StreamingTV'], prefix='StreamingTV')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'StreamingTV'
X_train = pd.get_dummies(X_train, columns=['StreamingMovies'], prefix='StreamingMovies')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['StreamingMovies'], prefix='StreamingMovies')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

import pandas as pd

# Perform one-hot encoding on 'Contract'
X_train = pd.get_dummies(X_train, columns=['Contract'], prefix='Contract')

# Print the column names after one-hot encoding
print(X_train.columns)

# Ensure consistent encoding with the training data
X_test = pd.get_dummies(X_test, columns=['Contract'], prefix='ContractContract')

# Align columns between training and testing sets in case of unseen categories
X_test = X_test.reindex(columns = X_train.columns, fill_value=0)

# Print the column names after one-hot encoding
print(X_test.columns)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'gender' column in the training data
X_train['PaperlessBilling_encoded'] = le.fit_transform(X_train['PaperlessBilling'])

# Transform the 'gender' column in the testing data
X_test['PaperlessBilling_encoded'] = le.transform(X_test['PaperlessBilling'])

import pandas as pd

# One-hot encode the `PaymentMethod` column in X_train
X_train = pd.get_dummies(X_train, columns=['PaymentMethod'], prefix='PaymentMethod')

# One-hot encode the `PaymentMethod` column in X_test, ensuring alignment with X_train
X_test = pd.get_dummies(X_test, columns=['PaymentMethod'], prefix='PaymentMethod')
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

from sklearn.preprocessing import StandardScaler

# Calculate the median of the 'tenure' column in X_train
median_tenure = X_train['tenure'].median()

# Create two new columns 'tenure_mode_low' and 'tenure_mode_high' in X_train
X_train['tenure_mode_low'] = (X_train['tenure'] <= median_tenure).astype(int)
X_train['tenure_mode_high'] = (X_train['tenure'] > median_tenure).astype(int)

# Standardize the 'tenure' column in X_train
scaler = StandardScaler()
X_train['tenure_scaled'] = scaler.fit_transform(X_train[['tenure']])

# Repeat for X_test using the median from X_train
X_test['tenure_mode_low'] = (X_test['tenure'] <= median_tenure).astype(int)
X_test['tenure_mode_high'] = (X_test['tenure'] > median_tenure).astype(int)
X_test['tenure_scaled'] = scaler.transform(X_test[['tenure']])
# Include all three features

import numpy as np
from sklearn.preprocessing import StandardScaler

# Apply log transformation (adding 1 to avoid log(0))
X_train['TotalCharges_log'] = np.log1p(X_train['TotalCharges'])
X_test['TotalCharges_log'] = np.log1p(X_test['TotalCharges'])

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the log-transformed column in X_train
X_train['TotalCharges_log_scaled'] = scaler.fit_transform(X_train[['TotalCharges_log']])

# Transform the log-transformed column in X_test
X_test['TotalCharges_log_scaled'] = scaler.transform(X_test[['TotalCharges_log']])

X_train.drop(columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',
       'PhoneService', 'PaperlessBilling', 'MonthlyCharges', 'TotalCharges','TotalCharges_log'],inplace=True)

X_test.drop(columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',
       'PhoneService', 'PaperlessBilling', 'MonthlyCharges', 'TotalCharges','TotalCharges_log'],inplace=True)

y_train = y_train.to_frame()
print(y_train.columns)

y_test = y_test.to_frame()
print(y_train.columns)

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the 'Churn' column in the training data
y_train['Churn_encoded'] = le.fit_transform(y_train['Churn'])

# Transform the 'Churn' column in the testing data
y_test['Churn_encoded'] = le.transform(y_test['Churn'])

y_train.drop(columns=['Churn'],inplace=True)

y_test.drop(columns=['Churn'],inplace=True)

"""# <u>Logistic Regression:</u>"""

clf = LogisticRegression(fit_intercept=True, max_iter=10000,class_weight='balanced')
clf.fit(X_train, y_train)

# Get the model coefficients
clf.coef_

clf.intercept_

"""<h1><u>Evaluating the performance of the trained model:</u></h1>"""

# Get the predicted probabilities
train_preds = clf.predict_proba(X_train)
test_preds = clf.predict_proba(X_test)

test_preds

train_preds = clf.predict_proba(X_train)[:,1]
test_preds = clf.predict_proba(X_test)[:,1]

test_preds

# Get the predicted classes
train_class_preds = clf.predict(X_train)
test_class_preds = clf.predict(X_test)

# Set NumPy print options to display all values
np.set_printoptions(threshold=np.inf)  # or threshold=sys.maxsize

# Now print the array
print(test_class_preds)

"""<h1> In this problem we will focus on minimizing false negatives (customers predicted to stay but actually churned).</h1>"""

# Get the accuracy scores
train_accuracy = accuracy_score(train_class_preds,y_train)
test_accuracy = accuracy_score(test_class_preds,y_test)

print("The accuracy on train data is ", train_accuracy)
print("The accuracy on test data is ", test_accuracy)

# Get the confusion matrix for both train and test

labels = ['Retained', 'Churned']
cm = confusion_matrix(y_train, train_class_preds)
print(cm)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax) #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

# Get the confusion matrix for both train and test

labels = ['Retained', 'Churned']
cm = confusion_matrix(y_test, test_class_preds)
print(cm)

ax= plt.subplot()
sns.heatmap(cm, annot=True, ax = ax); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels')
ax.set_ylabel('True labels')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(labels)
ax.yaxis.set_ticklabels(labels)

logistic = LogisticRegression()

scoring = ['accuracy']
scores = cross_validate(logistic,X_train, y_train, scoring = scoring, cv = 5, return_train_score=True,return_estimator=True,verbose = 10)

scores['train_accuracy']

scores['test_accuracy']

scores['estimator']

for model in scores['estimator']:
    print(model.coef_)

from sklearn.model_selection import learning_curve
# Assuming you have your model (model) and data (X_train, y_train)
train_sizes, train_scores, test_scores = learning_curve(
    model, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(.1, 1.0, 5)
)

# Calculate mean and standard deviation for training and testing scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label="Training score", color="navy")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="navy")
plt.plot(train_sizes, test_mean, label="Cross-validation score", color="darkorange")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="darkorange")
plt.title("Learning Curve")
plt.xlabel("Training Set Size")
plt.ylabel("Accuracy Score")
plt.legend(loc="best")
plt.show()

"""There is a mild overfitting so lets apply some regularization.

# **L2 regularization with logistic regression:**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, learning_curve, StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import matplotlib.pyplot as plt

# Assuming you have X_train, y_train, X_test, and y_test

# Hyperparameter Grid
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

# L2 Regularized Logistic Regression with GridSearchCV
model = LogisticRegression(penalty='l2', solver='liblinear')
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=0)
grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the best C value
best_C = grid_search.best_params_['C']

# Create a logistic regression model with the best C value
best_model = LogisticRegression(penalty='l2', C=best_C, solver='liblinear')

# --- Learning Curve ---

# Generate the learning curve data with the best model
train_sizes, train_scores, test_scores = learning_curve(
    best_model, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(.1, 1.0, 5), n_jobs=-1
)

# Calculate mean and standard deviation for training and testing scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label="Training score", color="navy")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="navy")
plt.plot(train_sizes, test_mean, label="Cross-validation score", color="darkorange")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="darkorange")
plt.title("Learning Curve")
plt.xlabel("Training Set Size")
plt.ylabel("Accuracy Score")
plt.legend(loc="best")
plt.show()

# --- Cross-validation and Coefficient Analysis ---

# Perform cross-validation with the best model, including scoring for precision, recall, and F1-score
scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}
scores = cross_validate(best_model, X_train, y_train, scoring=scoring, cv=5, return_train_score=True, return_estimator=True, verbose=10)

# Extract train and test scores for each fold and each metric
train_accuracy = scores['train_accuracy']
test_accuracy = scores['test_accuracy']
train_precision = scores['train_precision']
test_precision = scores['test_precision']
train_recall = scores['train_recall']
test_recall = scores['test_recall']
train_f1 = scores['train_f1']
test_f1 = scores['test_f1']

# Print the scores
print("Train accuracy for each fold:", train_accuracy)
print("Test accuracy for each fold:", test_accuracy)
print("Train precision for each fold:", train_precision)
print("Test precision for each fold:", test_precision)
print("Train recall for each fold:", train_recall)
print("Test recall for each fold:", test_recall)
print("Train F1-score for each fold:", train_f1)
print("Test F1-score for each fold:", test_f1)

# Get model coefficients for each fold
coefs = np.array([model.coef_ for model in scores['estimator']])
print("Coefficients for each fold:\n", coefs)

"""# **L1 regularization with logistic regression:**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, learning_curve, StratifiedShuffleSplit, cross_validate
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
import matplotlib.pyplot as plt

# Assuming you have X_train, y_train, X_test, and y_test

# Hyperparameter Grid - Reduced C values for stronger regularization
param_grid = {'C': [0.01, 0.1, 1, 10]}  # Removed very small and very large C values

# L1 Regularized Logistic Regression with StratifiedShuffleSplit
model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=10000, random_state=42)  # Increased max_iter
cv = StratifiedShuffleSplit(n_splits=5, test_size=0.25, random_state=0)
grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', return_train_score=True)
grid_search.fit(X_train, y_train)

# Get the best C value
best_C = grid_search.best_params_['C']

# --- Learning Curve ---

# Create a logistic regression model with the best C value
best_model = LogisticRegression(penalty='l1', C=best_C, solver='liblinear', max_iter=10000, random_state=42)  # Increased max_iter

# Generate the learning curve data with the best model
train_sizes, train_scores, test_scores = learning_curve(
    best_model, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(.1, 1.0, 5), n_jobs=-1
)

# Calculate mean and standard deviation for training and testing scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_mean, label="Training score", color="navy")
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color="navy")
plt.plot(train_sizes, test_mean, label="Cross-validation score", color="darkorange")
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color="darkorange")
plt.title("Learning Curve with L1 Regularization")
plt.xlabel("Training Set Size")
plt.ylabel("Accuracy Score")
plt.legend(loc="best")
plt.show()

# --- Evaluation on Test Set ---

# Train the model with the best hyperparameter (C=best_C) on the entire training set
clf = LogisticRegression(C=best_C, penalty='l1', solver='liblinear', fit_intercept=True, max_iter=10000, class_weight='balanced', random_state=42)
clf.fit(X_train, y_train)

# Make predictions on the held-out test set
y_pred = clf.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# --- Cross-validation and Coefficient Analysis ---

# Perform cross-validation with the best model, including scoring for precision, recall, and F1-score
scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}
scores = cross_validate(best_model, X_train, y_train, scoring=scoring, cv=5, return_train_score=True, return_estimator=True, verbose=10)

# Extract train and test scores for each fold and each metric
train_accuracy = scores['train_accuracy']
test_accuracy = scores['test_accuracy']
train_precision = scores['train_precision']
test_precision = scores['test_precision']
train_recall = scores['train_recall']
test_recall = scores['test_recall']
train_f1 = scores['train_f1']
test_f1 = scores['test_f1']

# Print the scores
print("Train accuracy for each fold:", train_accuracy)
print("Test accuracy for each fold:", test_accuracy)
print("Train precision for each fold:", train_precision)
print("Test precision for each fold:", test_precision)
print("Train recall for each fold:", train_recall)
print("Test recall for each fold:", test_recall)
print("Train F1-score for each fold:", train_f1)
print("Test F1-score for each fold:", test_f1)

# Get model coefficients for each fold
coefs = np.array([model.coef_ for model in scores['estimator']])
print("Coefficients for each fold:\n", coefs)

"""# **CONCLUSION**

Conclusion:

The Logistic Regression model with L1 regularization provides reasonable performance with an emphasis on high recall but low precision. The model is well-regularized and does not overfit. However, there is room to improve precision and balance the overall trade-off between false positives and true positives.



1. Model Performance Evaluation:
   Recall (0.8207): The recall score is high, indicating the model is doing a good job at capturing most of the actual positives. This high recall but low precision suggests that the model tends to classify many instances as positive, leading to more false positives but capturing more true positives.

2. Cross-validation Results:
   The L1 regularization has resulted in some feature coefficients being set to zero (feature elimination). This means some features are considered irrelevant by the model, which simplifies the model and helps reduce overfitting.
   The learning curve shows that adding more data improves model performance, as the training and cross-validation scores converge. This suggests that the model benefits from more data and might further improve with a larger dataset.

4. Bias-Variance Tradeoff:
   Bias: The gap between the training score (higher) and the cross-validation score (lower) is small, indicating that the model has a slight variance but not excessively. This means that the model has been regularized well and is not overly complex.
   Variance: Since the model isn't overfitting, it seems that the variance is under control.
"""